# Enhanced Co-training for Large Language Models

This repository contains the implementation for our ICML 2022 paper [Co-training Improves Prompt-based Learning for Large Language Models](https://arxiv.org/abs/2202.00828) and subsequent advancements, including tuning methodologies based on  [T-Few](https://github.com/r-three/t-few).

The code is instrumental for:
  - Enhancing the zero-shot and few-shot performance of large language models
  - Distilling large models like GPT-3 and T0 into compact task-specific models.

We sucesfully built many parts of this repository on top of the outstanding [T-Few](https://github.com/r-three/t-few) repository.

If you find this code useful, please consider citing our paper:

```
@inproceedin